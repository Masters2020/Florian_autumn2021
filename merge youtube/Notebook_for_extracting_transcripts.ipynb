{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t-1oJsj7wunU",
    "outputId": "67007c05-3645-4ee5-ac8b-918241374be1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: youtube_transcript_api in c:\\users\\boehm\\anaconda3\\envs\\thesis\\lib\\site-packages (0.4.3)\n",
      "Requirement already satisfied: requests in c:\\users\\boehm\\anaconda3\\envs\\thesis\\lib\\site-packages (from youtube_transcript_api) (2.26.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\boehm\\anaconda3\\envs\\thesis\\lib\\site-packages (from requests->youtube_transcript_api) (3.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\boehm\\anaconda3\\envs\\thesis\\lib\\site-packages (from requests->youtube_transcript_api) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\boehm\\anaconda3\\envs\\thesis\\lib\\site-packages (from requests->youtube_transcript_api) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\boehm\\anaconda3\\envs\\thesis\\lib\\site-packages (from requests->youtube_transcript_api) (2.0.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install youtube_transcript_api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_aF2Y-6nRbh1",
    "outputId": "ee424f98-3d85-4497-bf61-a9d593954409"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['04wPQu3BC2k', 'WAeBPskvP4Y', 'A8e9gvrZEps', 'HJG96xfyOk4', 'V0NGZcorqZ4', 'kj5ZEtCcnQk', 'VzM5X4udX1E', 'eadUggsvmNo', 'uYF3KLv0_3A', 'H4iwjnbHkCw', 'x5p18S_S8N0', 'cMLaJsSrZuQ', 'gslI7cu1C4M', 'tdcDGKXGTsg', '24d9BFtXPbc', '61gIgxFXy9U', 'QE3hR3rMHOI', '0uBYomUSOoM', 'pMy2Lf40Xts', 'sIBR4ZSnSFQ', '8eqtEw2AMTw', 'KvkbKVwYT1A', 'iymP2jq6nFs', 'dHQQm_sJHDA', 'bhMEqrpf5xM', 'cuZ6oA9MqhE', 'I8h7unfqCLw', 'uMIG00gPyns', 'yk-z-XHvNqM']\n",
      "29\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Mon May  4 11:12:28 2020\n",
    "\n",
    "@author: Siebe Albers\n",
    "\"\"\"\n",
    "## edited by Ivette Bonestroo 04-26-2021 \n",
    "\n",
    "#======================================================================== #\n",
    "'load a df with video ids (which will be used for the youtube api to download the transcripts: and later on for extracting the labels'  \n",
    "import pandas as pd                        \n",
    "dataset = pd.read_csv('autumn 2021.csv') \n",
    "#with open('IdList_selfWachtedYoutubeVids.txt', encoding=\"utf-8\") as f:\n",
    " #   idList2 = f.readlines() #txt file with the ids:\n",
    "idList = dataset['ID'].tolist()\n",
    "print(idList)\n",
    "print(len(idList))\n",
    "#alternatively:\n",
    "#dic = dict_oldDf # a dictionary where the keys correspond the the youtubeIDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_aF2Y-6nRbh1",
    "outputId": "ee424f98-3d85-4497-bf61-a9d593954409"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time it took: 13.939071416854858\n",
      "len trans 27\n",
      "len trans 0\n"
     ]
    }
   ],
   "source": [
    "#======================================================================== #\n",
    "' downloading the transcripts by their ids                           '\n",
    "#======================================================================== #\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "import time # just to record how long it takes to download the transcripts\n",
    "STARTTIME = time.time() #plus counting the time,\n",
    "Transcripts_w_timestamps1 =YouTubeTranscriptApi.get_transcripts(video_ids=idList,continue_after_error=True)\n",
    "\n",
    "Transcripts_w_timestamps = Transcripts_w_timestamps1[0]\n",
    "\n",
    "print('time it took:', time.time() - STARTTIME)\n",
    "\n",
    "print( 'len trans', len(Transcripts_w_timestamps)) # see how many could be downloaded\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# transcripts that were unable to be extraced:\n",
    "# =============================================================================\n",
    "#ids_thatcouldnotbedownloaded = list( set_originalId - set(downloadedtransIds )\n",
    "#print( 'len downloaded trans:',ids_thatcouldnotbedownloaded)\n",
    "\n",
    "not_extracted = Transcripts_w_timestamps1[1]\n",
    "Transcripts_w_timestamps2 =YouTubeTranscriptApi.get_transcripts(video_ids=not_extracted,continue_after_error=True, cookies = 'cookies.txt') \n",
    "## cookies txt is created based upon the cookies of these videos: \n",
    "##['areIv5h_vss', '4FN12sqoC4Y', 'UX5OwBnhksY', 'JYxJkq5B6ec', 'fMxFSfIV3Dc']\n",
    "## cookies.txt add-on for chrome is used to extract the txt. \n",
    "##The last two rows of the last four generated txtfiles are manually added to the first txtfile: 'cookies.txt'.\n",
    "Transcripts_w_timestamps2 = Transcripts_w_timestamps2[0]\n",
    "print( 'len trans', len(Transcripts_w_timestamps2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_aF2Y-6nRbh1",
    "outputId": "ee424f98-3d85-4497-bf61-a9d593954409"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# # creating a dict with transcripts, ψ Writing to string files to (re)create the transcripts\n",
    "# =============================================================================\n",
    "#create a list of video ids, serving as keys for next para\n",
    "IDLIST = list(Transcripts_w_timestamps.keys())\n",
    "IDLIST2 = list(Transcripts_w_timestamps2.keys())\n",
    "\n",
    "trans_dic_fromApi = {}\n",
    "for I in IDLIST:\n",
    "    TRANS = \"\"\n",
    "    trans_dic_fromApi[I] = None\n",
    "    for J in Transcripts_w_timestamps[I]:\n",
    "#        print(J['text'])\n",
    "        TRANS += J['text']\n",
    "        TRANS += \" \"\n",
    "    trans_dic_fromApi[I] = TRANS\n",
    "\n",
    "for I in IDLIST2:\n",
    "    TRANS = \"\"\n",
    "    trans_dic_fromApi[I] = None\n",
    "    for J in Transcripts_w_timestamps2[I]:\n",
    "#        print(J['text'])\n",
    "        TRANS += J['text']\n",
    "        TRANS += \" \"\n",
    "    trans_dic_fromApi[I] = TRANS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_aF2Y-6nRbh1",
    "outputId": "ee424f98-3d85-4497-bf61-a9d593954409"
   },
   "outputs": [],
   "source": [
    "#======================================================================== #\n",
    "' Exporting to disk         '\n",
    "#======================================================================== #\n",
    "import json\n",
    "with open('ourWatchedYoutubevidsTranscriptsψkeys.json', 'w') as fp:\n",
    "    json.dump(trans_dic_fromApi, fp)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## =============================================================================\n",
    "## #creating 1 dictionary for storing all meta Data of the transcripts\n",
    "## =============================================================================\n",
    "#a_meta_dict_transFromApi = { 'transcripts': {I: trans_dic_fromApi[I] for I in trans_dic_fromApi.keys() }, #look at the weird syntac I: ...[i] for i \n",
    "#         'lengths': {I: len(trans_dic_fromApi[I].split()) for I in trans_dic_fromApi.keys() },\n",
    "#         'labels': {I :OldTransDF_idsasKeys['label'][I] for I in trans_dic_fromApi.keys() } #extract labels from old df\n",
    "   #                      }\n",
    "#\n",
    "##check length of the transcript with same Id to see if it complies with lenlist with same id:\n",
    "#len ( a_meta_dict_transFromApi['transcripts']['-5RCmu-HuTg'].split() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X55EZzhMSkVt",
    "outputId": "9303ce1c-048e-4880-cab3-317daaa8eb69"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['iymP2jq6nFs', 'dHQQm_sJHDA']\n"
     ]
    }
   ],
   "source": [
    "print(Transcripts_w_timestamps1[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Tl97FAIsj01P"
   },
   "outputs": [],
   "source": [
    "def TranscriptExtractor():\n",
    "    \"This Function extracts a list from the dataset with the following string contents: [Video_ID, Video_Category, Video_Transcript, Video_Rating]\"\n",
    "    import os\n",
    "    import csv\n",
    "    import json\n",
    "    # first we load in all info from the youtube.csv we need\n",
    "   # os.chdir(directory_youtubecsv)\n",
    "    with open('autumn 2021.csv', mode='r') as infile:\n",
    "        reader = csv.reader(infile)\n",
    "        youtube_ranked = []\n",
    "        for rows in reader:\n",
    "            youtube_ranked.append(rows)\n",
    "        youtube_ranked_data = {}\n",
    "    for listed in youtube_ranked:\n",
    "        youtube_ranked_data.update({listed[0]: listed[3]})\n",
    "\n",
    "    with open('ourWatchedYoutubevidsTranscriptsψkeys.json') as f:\n",
    "        transcripts = json.load(f)\n",
    "    print(type(youtube_ranked))\n",
    "    id_cat_transcripts = []\n",
    "    id_transcripts = list(transcripts.keys())\n",
    "    for id in id_transcripts:\n",
    "        id_cat_transcripts.append([id, transcripts[id], youtube_ranked_data[id]])\n",
    "\n",
    "    return id_cat_transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "QVFPU9bCWlxB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "transcripts_labels = TranscriptExtractor()\n",
    "\n",
    "import csv\n",
    "with open('data_autumn2021.csv', 'w', newline='', encoding = 'utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"id\", \"transcript\", \"rating\"])\n",
    "with open(\"data_autumn2021.csv\", \"a\", newline=\"\", encoding = 'utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(transcripts_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Notebook_for_extracting_transcripts.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
